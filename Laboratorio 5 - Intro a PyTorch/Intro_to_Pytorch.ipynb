{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a PyTorch\n",
    "\n",
    "\n",
    "[PyTorch](https://pytorch.org/) es una biblioteca de aprendizaje profundo de código abierto basada en el lenguaje de programación Python. Desarrollada por Facebook's AI Research lab, PyTorch se ha convertido en una de las herramientas más populares en el campo de la inteligencia artificial y el aprendizaje automático. En esta notebook, vamos a explorar los conceptos básicos de PyTorch con su unidad de cálculo básica, el [tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html).\n",
    "\n",
    "Esta notebook esta basada en:\n",
    "\n",
    "- Joe Papa. (2021). _PyTorch Pocket Reference: Building and Deploying Deep Learning Models_. O'Reilly Media, Inc.\n",
    "- [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Qué es un tensor?\n",
    "\n",
    "Un [tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) es una matriz multidimensional que puede contener datos de diferentes tipos (por ejemplo, números enteros, flotantes, etc.). Los tensores son similares a los arreglos de NumPy, pero con la diferencia de que los tensores pueden ser utilizados en una GPU para acelerar los cálculos. En PyTorch, los tensores son la unidad básica de cálculo y se pueden crear de varias maneras.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8jdzMrA33Leu3j3F6A8a3w.png\" width=\"500\" alt=\"PyTorch logo\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de tensores\n",
    "\n",
    "Se pueden crear tensores de varias maneras en PyTorch. A continuación, se van a mostrar algunos ejemplos comunes de cómo crear tensores en PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde estructuras de datos\n",
    "\n",
    "Podemos crear un tensor a partir de una lista de Python, una tupla, una matriz de NumPy, etc. Para ello, podemos utilizar la función [torch.tensor()](https://pytorch.org/docs/stable/generated/torch.tensor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x = tensor([1, 2, 3])\n",
      " y = tensor([1, 2, 3])\n",
      " z = tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])  # creamos a partir de una lista\n",
    "y = torch.tensor((1, 2, 3))  # creamos a partir de una tupla\n",
    "z = torch.tensor(np.array([1, 2, 3]))  # creamos a partir de un array de numpy\n",
    "\n",
    "print(f\"{ x = }\")\n",
    "print(f\"{ y = }\")\n",
    "print(f\"{ z = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos con diferentes dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      " w = tensor(5)\n",
      "\n",
      " x = tensor([5, 4, 3])\n",
      "\n",
      " y = tensor([[5, 4, 3],\n",
      "        [2, 1, 0]])\n",
      "\n",
      " z = tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(5)  # 0 dimensiones\n",
    "x = torch.tensor([5, 4, 3])  # 1 dimension\n",
    "y = torch.tensor([[5, 4, 3], [2, 1, 0]])  # 2 dimensiones\n",
    "z = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # 3 dimensiones\n",
    "\n",
    "print(w)\n",
    "print(f\"{ w = }\\n\")\n",
    "print(f\"{ x = }\\n\")\n",
    "print(f\"{ y = }\\n\")\n",
    "print(f\"{ z = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores pueden contener datos de diferentes tipos, como enteros, flotantes, etc. **Lo importante es que todos los elementos de un tensor deben ser del mismo tipo**. Si no se especifica el tipo de datos, PyTorch inferirá el tipo de datos del tensor.\n",
    "Para saber todos los tipos de datos que se pueden utilizar en PyTorch, puedes consultar la [documentación oficial](https://pytorch.org/docs/stable/tensors.html#data-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x = tensor([1, 2, 3], dtype=torch.uint8)\n",
      " y = tensor([1., 2., 3.])\n",
      " z = tensor([False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.uint8)  # tipo de dato uint8\n",
    "y = torch.tensor([1, 2, 3], dtype=torch.float32)  # tipo de dato float32\n",
    "z = torch.tensor([False, True, False], dtype=torch.bool)  # tipo de dato booleano\n",
    "\n",
    "print(f\"{ x = }\")\n",
    "print(f\"{ y = }\")\n",
    "print(f\"{ z = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el tipo de datos de un tensor utilizando el atributo `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x.dtype = torch.uint8\n",
      " y.dtype = torch.float32\n",
      " z.dtype = torch.bool\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ x.dtype = }\")\n",
    "print(f\"{ y.dtype = }\")\n",
    "print(f\"{ z.dtype = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde tensores existentes\n",
    "\n",
    "Podemos crear un tensor a partir de un tensor existente utilizando el método [torch.clone()](https://pytorch.org/docs/stable/generated/torch.clone.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x = tensor([100.,   2.,   3.])\n",
      " y = tensor([100.,   2.,   3.])\n",
      " z = tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "y = x  # y apunta a la misma dirección de memoria que x\n",
    "z = x.clone()  # z apunta a una nueva dirección de memoria\n",
    "\n",
    "y[0] = 100  # cambia el valor de x\n",
    "\n",
    "print(f\"{ x = }\")\n",
    "print(f\"{ y = }\")\n",
    "print(f\"{ z = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: el método `clone()` es diferenciable, lo que significa que los gradientes se pueden propagar a través de él. Utiliza el método `detach()` si no quieres que los gradientes se propaguen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desde funciones\n",
    "\n",
    "Existen varias funciones en PyTorch que nos permiten crear tensores con valores específicos. Algunas de estas funciones son:\n",
    "\n",
    "- [torch.zeros()](https://pytorch.org/docs/stable/generated/torch.zeros.html): crea un tensor con todos los elementos establecidos en cero.\n",
    "- [torch.ones()](https://pytorch.org/docs/stable/generated/torch.ones.html): crea un tensor con todos los elementos establecidos en uno.\n",
    "- [torch.rand()](https://pytorch.org/docs/stable/generated/torch.rand.html): crea un tensor con valores aleatorios entre 0 y 1.\n",
    "- [torch.randn()](https://pytorch.org/docs/stable/generated/torch.randn.html): crea un tensor con valores aleatorios de una distribución normal.\n",
    "- [torch.arange()](https://pytorch.org/docs/stable/generated/torch.arange.html): crea un tensor con valores espaciados uniformemente dentro de un rango.\n",
    "- [torch.full()](https://pytorch.org/docs/stable/generated/torch.full.html): crea un tensor con todos los elementos establecidos en un valor específico.\n",
    "\n",
    "\n",
    "Muchos de estos métodos reciben como argumento la forma del tensor que queremos crear. Por ejemplo, si queremos crear un tensor de 2x3 con todos los elementos establecidos en cero, podemos hacerlo de la siguiente manera:\n",
    "\n",
    "```python\n",
    "torch.zeros(2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zeros = tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      " zeros = tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.uint8)\n",
      " zeros = tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros((2, 3))  # tensor de 2x3 con ceros (float32)\n",
    "print(f\"{ zeros = }\")\n",
    "\n",
    "zeros = torch.zeros((2, 3), dtype=torch.uint8)  # tensor de 2x3 con ceros (uint8)\n",
    "print(f\"{ zeros = }\")\n",
    "\n",
    "zeros = torch.zeros((2, 3, 4))  # tensor de 2x3 con ceros (float32)\n",
    "print(f\"{ zeros = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ones = tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(5)  # tensor de 5 elementos con unos\n",
    "print(f\"{ ones = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rand = tensor([[0.1734, 0.8624, 0.7162, 0.6757],\n",
      "        [0.7297, 0.0995, 0.9096, 0.4086],\n",
      "        [0.7447, 0.2317, 0.5437, 0.4078]])\n",
      " randn = tensor([[-2.3779,  1.4774, -0.5533,  1.7109],\n",
      "        [ 0.7120,  1.2065, -0.6737, -0.1267],\n",
      "        [ 1.2266, -1.7809, -2.1363, -0.1174]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(3, 4)  # tensor de 3x4 con valores aleatorios entre 0 y 1\n",
    "print(f\"{ rand = }\")\n",
    "\n",
    "randn = torch.randn(\n",
    "    3, 4\n",
    ")  # tensor de 3x4 con valores aleatorios de una distribución normal\n",
    "print(f\"{ randn = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arrange = tensor([ 0,  5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85,\n",
      "        90, 95])\n",
      " arrange = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
      "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "arrange = torch.arange(0, 100, 5)  # tensor con valores de 0 a 10 con paso de 2\n",
    "print(f\"{ arrange = }\")\n",
    "\n",
    "arrange = torch.arange(0, 100)  # tensor con valores de 0 a 10 con paso de 2\n",
    "print(f\"{ arrange = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " full_of_5 = tensor([[5, 5, 5],\n",
      "        [5, 5, 5],\n",
      "        [5, 5, 5]])\n"
     ]
    }
   ],
   "source": [
    "full_of_5 = torch.full((3, 3), 5)  # tensor de 3x3 con todos los elementos en 5\n",
    "print(f\"{ full_of_5 = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos de un tensor\n",
    "\n",
    "Los tensores en PyTorch tienen varios atributos que nos permiten acceder a información sobre el tensor. Algunos de los atributos más comunes son:\n",
    "\n",
    "- [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype): devuelve el tipo de datos del tensor.\n",
    "- [shape](https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html): devuelve la forma/dimensiones del tensor. También podemos utilizar el método [size()](https://pytorch.org/docs/stable/generated/torch.Tensor.size.html) para obtener la forma del tensor.\n",
    "- [ndim](https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html): devuelve el número de dimensiones del tensor. También podemos utilizar el método [dim()](https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html) para obtener el número de dimensiones del tensor.\n",
    "- [device](https://pytorch.org/docs/stable/generated/torch.Tensor.device.html): devuelve el dispositivo en el que se encuentra el tensor (CPU MPS o GPU).\n",
    "- [requires_grad](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html): indica si el tensor requiere cálculo de gradientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x.shape = torch.Size([3])\n",
      " x.size() = torch.Size([3])\n",
      "\n",
      " y.size() = torch.Size([2, 3])\n",
      " y.size(0) = 2\n",
      " y.size(1) = 3\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"{ x.shape = }\")\n",
    "print(f\"{ x.size() = }\\n\")\n",
    "\n",
    "print(\n",
    "    f\"{ y.size() = }\"\n",
    ")  # si no se pone argumento, devuelve una tupla con las dimensiones\n",
    "print(f\"{ y.size(0) = }\")  # devuelve el tamaño de la primera dimensión\n",
    "print(f\"{ y.size(1) = }\")  # devuelve el tamaño de la segunda dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x.ndim = 1\n",
      " x.dim() = 1\n",
      "\n",
      " y.ndim = 2\n",
      " y.dim() = 2\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"{ x.ndim = }\")\n",
    "print(f\"{ x.dim() = }\\n\")\n",
    "\n",
    "print(f\"{ y.ndim = }\")\n",
    "print(f\"{ y.dim() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.device = device(type='cpu')\n",
      "x movido a MPS\n",
      "x.device = device(type='mps', index=0)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "print(f\"{x.device = }\")  # cpu es el dispositivo por defecto\n",
    "\n",
    "if torch.cuda.is_available():  # si hay una GPU disponible (y cuda está instalado)\n",
    "    x = x.to(\"cuda\")  # movemos el tensor a la GPU\n",
    "    print(\"x movido a la GPU\")\n",
    "    print(f\"{x.device = }\")\n",
    "\n",
    "if (\n",
    "    torch.backends.mps.is_available()\n",
    "):  # si hay un MPS disponible (https://pytorch.org/docs/stable/mps.html)\n",
    "    x = x.to(\"mps\")\n",
    "    print(\"x movido a MPS\")\n",
    "    print(f\"{x.device = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es crear el tensor directamente en el dispositivo que queremos utilizando el argumento `device`. Por ejemplo, si queremos crear un tensor en la GPU, podemos hacerlo de la siguiente manera:\n",
    "\n",
    "```python\n",
    "torch.zeros(2, 3, device='cuda')\n",
    "```\n",
    "\n",
    "Es importante verificar que el dispositivo esté disponible antes de crear un tensor en él."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de reducción y agregación\n",
    "\n",
    "PyTorch proporciona varias funciones de reducción que nos permiten reducir un tensor a un solo valor. Algunas de las funciones de reducción más comunes son:\n",
    "\n",
    "- [torch.sum()](https://pytorch.org/docs/stable/generated/torch.sum.html): calcula la suma de todos los elementos de un tensor.\n",
    "- [torch.mean()](https://pytorch.org/docs/stable/generated/torch.mean.html): calcula la media de todos los elementos de un tensor.\n",
    "- [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html): calcula el valor máximo de un tensor.\n",
    "- [torch.min()](https://pytorch.org/docs/stable/generated/torch.min.html): calcula el valor mínimo de un tensor.\n",
    "- [torch.argmax()](https://pytorch.org/docs/stable/generated/torch.argmax.html): devuelve el índice del valor máximo de un tensor.\n",
    "- [torch.argmin()](https://pytorch.org/docs/stable/generated/torch.argmin.html): devuelve el índice del valor mínimo de un tensor.\n",
    "\n",
    "Estas funciones también aceptan un argumento `dim` que nos permite especificar la dimensión a lo largo de la cual queremos realizar la reducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.sum(x) = tensor(45.)\n",
      "torch.sum(x, dim=0) = tensor([12., 15., 18.])\n",
      "torch.sum(x, dim=1) = tensor([ 6., 15., 24.])\n",
      "\n",
      "torch.mean(x) = tensor(5.)\n",
      "torch.mean(x, dim=0) = tensor([4., 5., 6.])\n",
      "torch.mean(x, dim=1) = tensor([2., 5., 8.])\n",
      "\n",
      "torch.max(x) = tensor(9.)\n",
      "torch.max(x, dim=0) = torch.return_types.max(\n",
      "values=tensor([7., 8., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "torch.max(x, dim=1) = torch.return_types.max(\n",
      "values=tensor([3., 6., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "\n",
      "torch.min(x) = tensor(1.)\n",
      "torch.min(x, dim=0) = torch.return_types.min(\n",
      "values=tensor([1., 2., 3.]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "torch.min(x, dim=1) = torch.return_types.min(\n",
      "values=tensor([1., 4., 7.]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "\n",
      "torch.argmax(x) = tensor(8)\n",
      "torch.argmax(x, dim=0) = tensor([2, 2, 2])\n",
      "torch.argmax(x, dim=1) = tensor([2, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "print(f\"{torch.sum(x) = }\")  # suma de todos los elementos\n",
    "print(f\"{torch.sum(x, dim=0) = }\")  # suma de cada columna\n",
    "print(f\"{torch.sum(x, dim=1) = }\\n\")  # suma de cada fila\n",
    "\n",
    "print(f\"{torch.mean(x) = }\")  # media de todos los elementos\n",
    "print(f\"{torch.mean(x, dim=0) = }\")  # media de cada columna\n",
    "print(f\"{torch.mean(x, dim=1) = }\\n\")  # media de cada fila\n",
    "\n",
    "print(f\"{torch.max(x) = }\")  # máximo de todos los elementos\n",
    "print(f\"{torch.max(x, dim=0) = }\")  # máximo de cada columna\n",
    "print(f\"{torch.max(x, dim=1) = }\\n\")  # máximo de cada fila\n",
    "\n",
    "print(f\"{torch.min(x) = }\")  # mínimo de todos los elementos\n",
    "print(f\"{torch.min(x, dim=0) = }\")  # mínimo de cada columna\n",
    "print(f\"{torch.min(x, dim=1) = }\\n\")  # mínimo de cada fila\n",
    "\n",
    "print(\n",
    "    f\"{torch.argmax(x) = }\"\n",
    ")  # índice del máximo de todos los elementos, como es una matriz, devuelve el índice en el array plano\n",
    "print(f\"{torch.argmax(x, dim=0) = }\")  # índice del máximo de cada columna\n",
    "print(f\"{torch.argmax(x, dim=1) = }\\n\")  # índice del máximo de cada fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación y división\n",
    "\n",
    "PyTorch proporciona varias funciones que nos permiten combinar y dividir tensores. Algunas de las funciones más comunes son:\n",
    "\n",
    "- [torch.cat()](https://pytorch.org/docs/stable/generated/torch.cat.html): combina varios tensores a lo largo de una dimensión específica.\n",
    "- [torch.stack()](https://pytorch.org/docs/stable/generated/torch.stack.html): apila varios tensores a lo largo de una nueva dimensión.\n",
    "- [torch.split()](https://pytorch.org/docs/stable/generated/torch.split.html): divide un tensor en varias partes a lo largo de una dimensión específica.\n",
    "- [torch.chunk()](https://pytorch.org/docs/stable/generated/torch.chunk.html): divide un tensor en varias partes a lo largo de una dimensión específica.\n",
    "- [torch.squeeze()](https://pytorch.org/docs/stable/generated/torch.squeeze.html): elimina dimensiones de tamaño 1 de un tensor.\n",
    "- [torch.unsqueeze()](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html): agrega una dimensión de tamaño 1 a un tensor.\n",
    "- [torch.reshape()](https://pytorch.org/docs/stable/generated/torch.reshape.html): cambia la forma de un tensor.\n",
    "\n",
    "Existen muchas más funciones que nos permiten combinar y dividir tensores en PyTorch. Puedes consultar la [documentación oficial](https://pytorch.org/docs/stable/torch.html) para obtener más información sobre estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cat((x, y)) = tensor([1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0])\n",
      "\n",
      "torch.stack((x, y)) = tensor([[1, 2, 3, 4, 5, 6],\n",
      "        [5, 4, 3, 2, 1, 0]])\n",
      "\n",
      "torch.split(x, 2) = (tensor([1, 2]), tensor([3, 4]), tensor([5, 6]))\n",
      "\n",
      "torch.chunk(x, 2) = (tensor([1, 2, 3]), tensor([4, 5, 6]))\n",
      "\n",
      "torch.squeeze(torch.zeros(1, 2, 1, 2)) = tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "torch.unsqueeze(torch.zeros(2, 2), 0) = tensor([[[0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "y = torch.tensor([5, 4, 3, 2, 1, 0])\n",
    "\n",
    "print(f\"{torch.cat((x, y)) = }\\n\")  # concatenación de tensores\n",
    "print(f\"{torch.stack((x, y)) = }\\n\")  # apilamiento de tensores\n",
    "print(f\"{torch.split(x, 2) = }\\n\")  # división de un tensor en partes de tamaño 2\n",
    "print(f\"{torch.chunk(x, 2) = }\\n\")  # división de un tensor en 2 partes\n",
    "\n",
    "print(\n",
    "    f\"{torch.squeeze(torch.zeros(1, 2, 1, 2)) = }\\n\"\n",
    ")  # elimina las dimensiones de tamaño 1\n",
    "print(f\"{torch.unsqueeze(torch.zeros(2, 2), 0) = }\")  # añade una dimensión al principio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.reshape(x, (3, 3)) = tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "torch.reshape(x, (3, -1)) = tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "torch.reshape(x, (-1, 3)) = tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "torch.reshape(x, (2, 3)) = tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "torch.reshape(x, (3, 2)) = tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "torch.reshape(x, (6, -1)) = tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9)\n",
    "print(f\"{torch.reshape(x, (3, 3)) = }\\n\")  # reorganiza los elementos en una matriz 3x3\n",
    "print(\n",
    "    f\"{torch.reshape(x, (3, -1)) = }\\n\"\n",
    ")  # -1 significa que se infiere el tamaño de esa dimensión\n",
    "print(f\"{torch.reshape(x, (-1, 3)) = }\\n\")\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(f\"{torch.reshape(x, (2, 3)) = }\\n\")  # reorganiza los elementos en una matriz 2x3\n",
    "print(f\"{torch.reshape(x, (3, 2)) = }\\n\")  # reorganiza los elementos en una matriz 3x2\n",
    "print(f\"{torch.reshape(x, (6, -1)) = }\\n\")  # reorganiza los elementos en una matriz 6x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de gradientes\n",
    "\n",
    "PyTorch proporciona una forma sencilla de calcular gradientes utilizando el módulo [torch.autograd](https://pytorch.org/docs/stable/autograd.html). Este módulo nos permite calcular gradientes automáticamente utilizando el método de retropropagación. Para ello, debemos establecer el atributo `requires_grad` en `True` al crear un tensor. Esto indica que queremos calcular los gradientes para este tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([2., 3.], requires_grad=True)\n",
      "y = tensor(6., grad_fn=<MulBackward0>)\n",
      "x.grad = tensor([3., 2.])\n"
     ]
    }
   ],
   "source": [
    "# 1) Creamos un tensor con dos valores y activamos el cálculo de gradientes\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"{x = }\")\n",
    "\n",
    "# 2) Definimos una función simple:\n",
    "#    y = x1 * x2\n",
    "y = x[0] * x[1]\n",
    "print(f\"{y = }\")\n",
    "\n",
    "# 3) Calculamos gradientes: esto rellena x.grad con dy/dx\n",
    "y.backward()\n",
    "print(f\"{x.grad = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente $\\nabla_x y$ es un vector que nos dice **cuánto cambia $y$** si cambiamos un poquito **cada componente** de $x$.  \n",
    "\n",
    "- $\\frac{\\partial y}{\\partial x_1} = x_2$.\n",
    "- $\\frac{\\partial y}{\\partial x_2} = x_1$.\n",
    "\n",
    "Por eso PyTorch nos devuelve `x.grad = [3., 2.]`.\n",
    "\n",
    "- Si aumento **solo** $x_1$ en una pequeña cantidad $\\delta$, el producto $y$ aumentará aproximadamente $x_2 \\times \\delta$.  \n",
    "  Aquí $x_2=3$, así que el “efecto” de subir $x_1$ es multiplicado por 3.  \n",
    "- De forma análoga, subir $x_2$ hace que $y$ cambie en aproximadamente $x_1 \\times \\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hur5OGlgj-Ct"
   },
   "source": [
    "## CPU/GPU/MPS\n",
    "\n",
    "PyTorch es compatible con varios dispositivos, como CPU, GPU y MPS (Metal Performance Shaders). Podemos mover tensores entre dispositivos utilizando el método [to()](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html). También podemos utilizar el método [cuda()](https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html) para mover un tensor a la GPU y el método [cpu()](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html) para mover un tensor a la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpnJwoJPjiOE",
    "outputId": "e21f35b0-4b0b-43ef-80e4-c842418cb66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbvuXJQAosTl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time:\n",
      "41.2 ms ± 1.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      " \n",
      "MPS time:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mMPS time:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtorch.mm(x,y)+z\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/laboratorio/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2486\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2484\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2485\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2486\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2489\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2490\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/laboratorio/lib/python3.12/site-packages/IPython/core/magics/execution.py:1229\u001b[39m, in \u001b[36mExecutionMagics.timeit\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1226\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m time_number >= \u001b[32m0.2\u001b[39m:\n\u001b[32m   1227\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1229\u001b[39m all_runs = \u001b[43mtimer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1230\u001b[39m best = \u001b[38;5;28mmin\u001b[39m(all_runs) / number\n\u001b[32m   1231\u001b[39m worst = \u001b[38;5;28mmax\u001b[39m(all_runs) / number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/laboratorio/lib/python3.12/timeit.py:208\u001b[39m, in \u001b[36mTimer.repeat\u001b[39m\u001b[34m(self, repeat, number)\u001b[39m\n\u001b[32m    206\u001b[39m r = []\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     r.append(t)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/laboratorio/lib/python3.12/site-packages/IPython/core/magics/execution.py:183\u001b[39m, in \u001b[36mTimer.timeit\u001b[39m\u001b[34m(self, number)\u001b[39m\n\u001b[32m    181\u001b[39m gc.disable()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     timing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<magic-timeit>:1\u001b[39m, in \u001b[36minner\u001b[39m\u001b[34m(_it, _timer)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if torch.cpu.is_available():\n",
    "    x = torch.rand(2, 900000).cpu()            # Initialize with random number (uniform distribution)\n",
    "    y = torch.randn(900000,200).cpu()           # With normal distribution (SD=1, mean=0)\n",
    "    z = torch.randperm(200).cpu()           # Size 200. Random permutation of integers from 0 to 200\n",
    "\n",
    "    print('CPU time:')\n",
    "    %timeit torch.mm(x,y)+z\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.rand(2, 900000).cuda()            \n",
    "    y = torch.randn(900000,200).cuda()          \n",
    "    z = torch.randperm(200).cuda()\n",
    "\n",
    "    print(' ')\n",
    "    print('GPU time:')\n",
    "    %timeit torch.mm(x,y)+z\n",
    "    \n",
    "if torch.backends.mps.is_available():\n",
    "    x = torch.rand(2, 900000).to(\"mps\")            \n",
    "    y = torch.randn(900000,200).to(\"mps\")          \n",
    "    z = torch.randperm(200).to(\"mps\")\n",
    "\n",
    "    print(' ')\n",
    "    print('MPS time:')\n",
    "    %timeit torch.mm(x,y)+z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nl8qO_Sf2WZ1"
   },
   "source": [
    "# FeedForward networks\n",
    "\n",
    "Son la unidad más simple de red neuronal, con su origen en el perceptron de muchas capas. La idea es crear una secuencia lineal de neuronas (capa) que reciben nuestro input. \n",
    "\n",
    "![Image](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)\n",
    "\n",
    "De esta manera la primera capa de neuronas (input layer) recibe los datos y las capas subsiguientes reciben el resultados de capas anteriores. La última capa (output layer) es la encargada de generar una predicción a partir de nuestros inputs.\n",
    "\n",
    "***\n",
    "\n",
    "En este notebook vamos a usar un dataset muy simple y conocido de imágenes, Fashion-MNIST. Se trata de un dataset de ropa y calzado, la idea es usar redes neuronales para clasificar cada una de las imágenes el tipo de ropa que representa. \n",
    "\n",
    "Para trabajar con imagenes vamos a hacer uso de una librería complementaria a Pytorch: **torchvision** (https://pytorch.org/docs/stable/torchvision/index.html) que incluye varios datasets precargados, modelos preentrenados y algunas utilidades para trabajar con imágenes que nos van a resultar útiles.\n",
    "\n",
    "*** \n",
    "\n",
    "En la celda de abajo vamos a carga nuestro dataset y mostrar algunas imagenes de ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "JpVIgjO52Uou",
    "outputId": "1efde127-8277-401d-af9d-d7bfb3129a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset 60000 imagenes.\n",
      "Clases posibles: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "Objeto imagen: <PIL.Image.Image image mode=L size=28x28 at 0x14C9288C0> - Clase 9\n",
      "Detalles de la imagen (28, 28) pixeles\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_PATH = \"./data\"  # ruta donde se guardan los datos\n",
    "\n",
    "mnist_dataset = dsets.FashionMNIST(MNIST_PATH, download=True)\n",
    "\n",
    "print(f\"Tamaño del dataset {len(mnist_dataset)} imagenes.\")\n",
    "print(f\"Clases posibles: {mnist_dataset.classes}\")\n",
    "\n",
    "data_idx = 0  # Indice (0-59999) de la imagen que queremos ver\n",
    "image, label = mnist_dataset[0] \n",
    "\n",
    "print(f\"Objeto imagen: {image} - Clase {label}\")\n",
    "print(f\"Detalles de la imagen {image.size} pixeles\")\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objeto imagen: <PIL.Image.Image image mode=L size=28x28 at 0x1613A68D0> - Clase 0\n",
      "Detalles de la imagen (28, 28) pixeles\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = mnist_dataset[1] \n",
    "\n",
    "print(f\"Objeto imagen: {image} - Clase {label}\")\n",
    "print(f\"Detalles de la imagen {image.size} pixeles\")\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peWiTvQq2Uua"
   },
   "source": [
    "### Clasificador\n",
    "\n",
    "Ahora que tenemos una idea de como es nuestro dataset, vamos a crear un modelo FeedForward para predecir la clase de la imagen que usemos como input. \n",
    "\n",
    "Antes que nada, vamos a necesitar dividir el dataset total en conjuntos de **entrenamiento**, **validacion** y **test**. Vamos a usar un ratio de 80 y 20% respectivamente. El set de test se puede descargar por separado con torchvision. Además, vamos a necesitar una manera de cargar **batches** de datos a la vez, para entrenar nuestra red. Pytorch nos proporciona varias ayudas para esto.\n",
    "\n",
    "***\n",
    "\n",
    "Finalmente, queda aclarar el uso de **tranformaciones** sobre las imágenes. Por lo pronto, tenemos objetos de tipo PIL Image, necesitamos (al menos) convertirlos en Tensores, para que Pytorch los pueda manejar.\n",
    "\n",
    "Hay un numero inmenso de transformaciones posibles que podemos usar en nustras imagenes, en este caso basta con tranformarlas a tensores, pero dejamos este link para otros casos: https://pytorch.org/docs/stable/torchvision/transforms.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto nos permite cambiarle la forma a un tensor aplicandole una transformacion. \n",
    "\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return torch.reshape(img, self.new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVBOfbYX2UzC"
   },
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(), ReshapeTransform((-1,))])\n",
    "\n",
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(MNIST_PATH, download=True, train=True, transform=img_transforms)\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_size = int(0.8 * len(mnist_train_dataset))\n",
    "val_size = len(mnist_train_dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [train_size, val_size])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(MNIST_PATH, download=True, train=False, transform=img_transforms)\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKgtfpeArQPd"
   },
   "source": [
    "### Modelo\n",
    "\n",
    "Vamos a considerar cada imagen como un tensor de una sola dimensión, de largo 28*28 = 784. Cada uno de esos valores representa el valor de un pixel de nuestra imagen original.\n",
    "\n",
    "Nuestra red va a recibir ese tensor como input (en realidad, un batch de tensores de largo 784) que va a ser trabajado por varias capas ocultas con diferente número de neuronas hasta llegar a una capa de salida con 10 outputs, 1 por cada clase posible.\n",
    "\n",
    "***\n",
    "\n",
    "Vamos utilizar capas conectadas totalmente, tambien conocidas como Fully Connected, Dense, o Linear en Pytorch (https://pytorch.org/docs/stable/nn.html). Para crearlas necesitamos especificar las dimensiones del tensor de entrada, y el de salida; luego internamente Pytorch genera la matriz de pesos por los cuales multiplicar la entrada para generar la salida. Luego de cada una de estas operaciones necesitamos usar una funcion de activacion no linear, en este caso, vamos a usar ReLU: https://pytorch.org/docs/stable/nn.html#relu. \n",
    "\n",
    "***\n",
    "\n",
    "Para implementar un modelo **cualquiera** alcanza con definir un metodo **init** donde especificamos la arquitectura del mismo, y un método **forward** donde especificamos cómo interactúan nuestras capas frente a un nuevo input.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "h-SZey-Qqj1P",
    "outputId": "c3466114-d313-4b5c-c589-e6027ac4fe6d"
   },
   "outputs": [],
   "source": [
    "# Definicion del modelo que vamos a usar. En Pytorch los modelos se definen como clases, que heredan de nn.Module\n",
    "\n",
    "class FeedForwardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, number_classes=10):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=128) #784 pixeles -> 128 neuronas\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=64) #128 neuronas -> 64\n",
    "        self.output = nn.Linear(in_features=64, out_features=number_classes) # 64 -> 10\n",
    "        #EL output es un tenser con 10 numeros (tensor(1, 2, 3, ..., 10))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # x = x.view(x.size(0), -1)  # aplana la imagen\n",
    "        # le decimos que pase nuestro batch de imagenes por la primer capa\n",
    "        x = F.relu(self.linear1(x))\n",
    "        # pasan por la segunda capa (linear 2)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        # pasoan por untima capa\n",
    "        logits = self.output(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = FeedForwardModel(number_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYv_kROa0o54"
   },
   "source": [
    "### Entrenando el modelo\n",
    "\n",
    "Para entrenar un modelo necesitamos una funcion de costo o pérdida (normalmente referida como loss function: https://pytorch.org/docs/stable/nn.html#loss-functions). En este curso no nos vamos a meter en mucho detalle sobre las funciones de costo, para este ejercicio y el siguiente vamos a usar la CrossEntropyLoss, y cuando necesiten otra la vamos a especificar.\n",
    "\n",
    "El objetivo de esta funcion es darnos un valor de que tan malas fueron las predicciones del modelo respecto a los valores de verdad. Haciendo uso de backpropagation y del gradiente de esta funcion podemos optimizar los pesos de nuestra red tal que \"aprenda\" a hacer mejores predicciones. De nuevo, la lógica detras de toda esta optimización no nos compete en este curso y lo dejamos para la disciplina de Deep Learning.\n",
    "\n",
    "***\n",
    "Como mencionamos arriba, el costo de computa usando las predicciones del modelo y las etiquetas verdaderas de nuestros datos y, el trabajo de actualizar los pesos usando los gradientes lo realiza un optimizador de Pytorch: https://pytorch.org/docs/stable/optim.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYtCjd9cqj3x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(DEVICE)\n",
    "ff_model = FeedForwardModel(number_classes=10).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "ff_optimizer = optim.SGD(ff_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "zBEV-LNsqj6o",
    "outputId": "7965009c-f8c9-420c-c3b0-0a290420156b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_func, optimizer, epochs):\n",
    "    for _ in range(epochs):  # Iteramos sobre el dataset entero muchas veces\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Reseteamos los gradientes de los pesos del modelo.\n",
    "            optimizer.zero_grad()   \n",
    "\n",
    "            # Obtenemos las predicciones para las nuevas imagenes llamando a nuestro modelo.\n",
    "            predictions = model(images)    \n",
    "\n",
    "            # Calulamos el costo de nuestras predicciones respecto a la verdad\n",
    "            loss = loss_func(predictions, labels)\n",
    "\n",
    "            # Computamos los gradientes con backward y actualizamos los pesos con un optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Luego de cada epoch de entrenamiento vemos la performance (accuracy) en el set de validacion\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0.0\n",
    "\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                predictions = model(images)\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "                correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "        print(f\"Validation accuracy {(100 * correct_predictions / len(val_loader.dataset)):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "leKy45f4N0q8",
    "outputId": "24580975-aa42-4f23-9f2c-3f76e9a25075"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    # Finalmente Reportamos la performance en el test set:\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            predictions = model(images)\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "    print(f\"Test set accuracy {(100 * correct_predictions / len(test_loader.dataset)):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy 22.23 %\n",
      "Validation accuracy 32.51 %\n",
      "Validation accuracy 42.94 %\n",
      "Validation accuracy 54.83 %\n",
      "Validation accuracy 58.77 %\n",
      "Validation accuracy 61.81 %\n",
      "Validation accuracy 63.42 %\n",
      "Validation accuracy 64.26 %\n",
      "Validation accuracy 65.47 %\n",
      "Validation accuracy 66.67 %\n",
      "Validation accuracy 67.22 %\n",
      "Validation accuracy 69.07 %\n",
      "Validation accuracy 68.97 %\n",
      "Validation accuracy 71.37 %\n",
      "Validation accuracy 71.57 %\n"
     ]
    }
   ],
   "source": [
    "# Usando las funciones definidas arriba entrenar un modelo es trivial\n",
    "train_model(ff_model, train_loader, val_loader, loss_func=criterion, optimizer=ff_optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy 70.31 %\n"
     ]
    }
   ],
   "source": [
    "test_model(ff_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Convolucionales\n",
    "\n",
    "\n",
    "![Image](https://www.unite.ai/wp-content/uploads/2019/12/Typical_cnn-1.png)\n",
    "\n",
    "\n",
    "Las redes convolucionales (CNNs) se basan en el uso de una técnica muy usada en el campo de computer vision tradicional, las [convoluciones](https://en.wikipedia.org/wiki/Kernel_\\(image_processing\\)) , la idea es crear un filtro pequeño que pasamos por encima de toda la imagen y nos permite detectar distintos elementos (como son líneas verticales, horizontales, diagonales, circulos, etc). EL gran problema de las convoluciones es que para crear dichos filtros debemos poder especificar distintos valores (pesos) para cada región en el mismo. \n",
    "\n",
    "Cada filtro (tambien conocido como kernel) nos permite identificar algo en particular en la imágen, y aplicar un filtro al resultado de otro (u otros) nos permite obtener informacion de más alto nivel (como por ejemplo detectar ojos, ruedas, puertas, etc).\n",
    "\n",
    "![Image](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "***\n",
    "\n",
    "Las redes convolucionales nos dan una manera de no sólo aprender los valores óptimos para dichos filtros (mediante backprop) sino tambien la posibilidad de hacerlo a escala usndo un número arbitrario de los mismos. Una gran ventaja que nos trae el uso de filtros, es el hecho de que requieren de un número muy chico de pesos a entrenar, lo que reduce el tamaño de nuestra red y nos permite entrenar mas rápido (o redes mas grandes y profundas con el mismo hardware).\n",
    "\n",
    "Una cosa a notar en las redes convolucionales es el hecho de que las imágenes se van reduciendo en su tamaño a medida que fluyen por la red, esto se debe a la opeación de `maxpooling` que toma regiones (por lo general de 2x2) en nuestra imagen y se queda con el valor más alto en la zona, reduciendo asi el tamaño de la imagen. El resultado de aplicar un filtro de convolución a una imagen se llama `feature_map` y se puede pensar como otra imagen que describe la características de la original. \n",
    "\n",
    "***\n",
    "\n",
    "Al final de nuestra red, necesitamos formar una predicción de la clase de nuestra imagen, por lo que tenemos que \"achatar\" estos feature maps y pasarlos por una (o varias) capas lineales que generen una predicción. Esto se puede ver como representar toda la informacion que conocemos de la imagen, como por ejemplo si tiene nariz, orejas, pelo, en un sólo vector; y decidir que ese vector representa a un perro.\n",
    "\n",
    "***\n",
    "\n",
    "Visualización de la red convolucional: https://adamharley.com/nn_vis/cnn/2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(MNIST_PATH, download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_size = int(0.8 * len(mnist_train_dataset))\n",
    "val_size = len(mnist_train_dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [train_size, val_size])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(MNIST_PATH, download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Convolucional\n",
    "\n",
    "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arquitectura y comportamiento de los componentes del modelo. \n",
    "\n",
    "En particular vamos a usar:\n",
    "\n",
    "- capas convolucionales de 2D (https://pytorch.org/docs/stable/nn.html#conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes). \n",
    "\n",
    "- Capas de maxpooling (https://pytorch.org/docs/stable/nn.html#maxpool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
    "\n",
    "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self, number_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
    "        \n",
    "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
    "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
    "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=7*7*8, out_features=128) \n",
    "        \n",
    "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, new_input):\n",
    "        # new_input shape: (batch_size, 1, 28, 28)\n",
    "        result = self.conv1(new_input)\n",
    "        # result shape: (batch_size, 16, 28, 28)\n",
    "        result = F.relu(self.pooling_layer(result))\n",
    "        # result shape: (batch_size, 16, 14, 14)\n",
    "        \n",
    "        result = self.conv2(result)\n",
    "        # result shape: (batch_size, 8, 14, 14)\n",
    "        result = F.relu(self.pooling_layer(result))     \n",
    "        # result shape: (batch_size, 8, 7, 7)\n",
    "        \n",
    "        # \"Achatamos\" los feature maps\n",
    "        result = result.view(result.size(0), -1)\n",
    "        result = F.relu(self.fc1(result))\n",
    "        \n",
    "        return self.output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tareas\n",
    "\n",
    "- [extra] Mejorar el modelo de la red convolucional.\n",
    "- [extra] Probar con otros datasets de imagenes. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "laboratorio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "45df4caade0db3d6696bb228726ea350dcb27de05c3271069f289aeea2b6e102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
