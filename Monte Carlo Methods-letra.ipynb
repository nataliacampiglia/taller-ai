{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tarea 3 - Métodos Monte Carlo (blackjack)\n",
    "\n",
    "En este laboratorio vamos a explorar los métodos de Monte Carlo (Capítulo 5 del libro de Sutton y Barto). Para esto, vamos a volver a utilizar un ambiente definido en [Gymnasium](https://gymnasium.farama.org/index.html), es el caso esta vez de otro ambiente sencillo: el juego de Blackjack. El [ambiente de gymnasium](https://gymnasium.farama.org/environments/toy_text/blackjack/) esta inspirado por el ejemplo 5.1 del libro, ya esta incluido en gym, por lo que no es necesario crearlo desde cero. \n",
    "\n",
    "\n",
    "## Descripción del ambiente a usar\n",
    "\n",
    "### Reglas del juego\n",
    "\n",
    "Es un juego de cartas donde el objetivo es obtener cartas que sumen lo más cercano a 21 posible, sin pasarnos. Jugamos contra un dealer fijo unicamente.\n",
    "\n",
    "Reglas:\n",
    "   \n",
    "- Las cartas con figuras (Jotas, Reinas y Reyes) tienen valor de 10.\n",
    "- Los Ases pueden valer 11 ó 1, cuando vale 11 se lo llama \"usable\" y es su valor por defecto.\n",
    "- En este caso jugamos con un mazo infinito (con reemplazo).\n",
    "- El dealer comienza con una carta boca arriba y una boca abajo.\n",
    "- El jugador puede pedir una carta (HIT) hasta que decida quedarse (STICK) o exceeda los 21 puntos (BUST).\n",
    "- Cuando el jugador se queda (STICK), el dealer muestra su carta boca abajo y pide cartas hasta que su suma sea 17 o más.\n",
    "- Si el dealer se pasa de 21, el jugador gana. En caso contrario, gana quien tenga la suma más cerca de 21.\n",
    "\n",
    "### Implementación en Gymnasium\n",
    "\n",
    "- La reward por perder es -1, por ganar es +1 y por pedir carta es 0. En caso de empate, la reward es 0.\n",
    "- Cada observacion es una tupla que tiene: \n",
    "    - la suma del jugador\n",
    "    - la carta boca arriba del dealer (1-10 donde 1 es un As)\n",
    "    - True o False si el jugador tiene un As usable o no\n",
    "\n",
    "Revisa el [ambiente en gymnasium](https://gymnasium.farama.org/environments/toy_text/blackjack/) para entender como funciona.\n",
    "\n",
    "\n",
    "### A entregar\n",
    "\n",
    "- Notebook con solución a los algoritmos presentados\n",
    "- Analisis de la función de valor estimada para distinto número de episodios a visitar (ej: 100, 10000, 50000, 500000). Se busca que puedan demostrar entendimiento del algoritmo y sus resultados (Esperamos gráficas y analisis de las mismas). \n",
    "- Estimaciones de función de valor para otras politicas definidas por el estudiante (al menos una). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from Utils import plot_value_distribution, plot_Q_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos asegurams que las gráficas se muestren en el notebook\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vemos el espacio de estado y acción del ambiente de blackjack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "# Crear el entorno de Blackjack\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jugador: 21, Dealer: 4, As usable: 1 \n"
     ]
    }
   ],
   "source": [
    "# vemos un ejemplo de observación\n",
    "obs, _ = env.reset()\n",
    "print(f\"Jugador: {obs[0]}, Dealer: {obs[1]}, As usable: {obs[2]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulemos una partida de blackjack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: (5, 9, 0)\n",
      "state: (14, 9, 0) reward: 0.0 done: False\n",
      "state: (14, 9, 0) reward: -1.0 done: True\n",
      "Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Caso de prueba\n",
    "state,_ = env.reset()\n",
    "print('initial state:', state)\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print('state:', state, 'reward:', reward, 'done:', done)\n",
    "    if done or truncated:\n",
    "        print(f'Reward: {reward}')                \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de la función de valor ($V$ y $Q$)\n",
    "\n",
    "Vamos a implementar el algoritmo de Monte Carlo para estimar las funciones de valor $V$ y $Q$ para el ambiente de blackjack asi como algunas variantes (first visit, every visit, exploración inicial, etc).\n",
    "\n",
    "Para ello primero vamos a definir una politica: Si suma 20 o más, se queda (STICK), si no pide carta (HIT). Esta política es la que se usa en el ejemplo del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STICK = 0\n",
    "HIT = 1\n",
    "       \n",
    "## Politica Sutton & Barto\n",
    "def sutton_policy(observation):\n",
    "    sum, _, _ = observation\n",
    "    return STICK if sum >= 20 else HIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de episodios\n",
    "\n",
    "Vamos a implementar una función que genere episodios de la política definida. La función va a recibir el ambiente y la política y va a devolver una lista de transiciones (estado, acción, reward, next_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy, env):\n",
    "    \"\"\"\n",
    "    Genera un episodio utilizando la política dada.\n",
    "    Args:\n",
    "        policy: Función que toma un estado y devuelve una acción.\n",
    "        env: Entorno de OpenAI Gym.\n",
    "    Returns:\n",
    "        episode: Lista de tuplas (estado, acción, recompensa, siguiente_estado).\n",
    "    \"\"\"\n",
    "    episodio = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        new_state, reward, terminated, truncated, _=  env.step(action)\n",
    "        episodio.append((state, action, reward, new_state))\n",
    "        done = terminated or truncated\n",
    "        state = new_state\n",
    "    \n",
    "    return episodio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio generado:\n",
      "Estado: (10, 5, 0), Acción: 1 (Hit), Recompensa: 0.0, Siguiente estado: (17, 5, 0)\n",
      "Estado: (17, 5, 0), Acción: 1 (Hit), Recompensa: 0.0, Siguiente estado: (21, 5, 0)\n",
      "Estado: (21, 5, 0), Acción: 0 (Stick), Recompensa: 1.0, Siguiente estado: (21, 5, 0)\n"
     ]
    }
   ],
   "source": [
    "# lo probamos\n",
    "episode = generate_episode(sutton_policy, env)\n",
    "print(\"Episodio generado:\")\n",
    "for state, action, reward, next_state in episode:\n",
    "    print(f\"Estado: {state}, Acción: {action} ({'Hit' if action == 1 else 'Stick'}), Recompensa: {reward}, Siguiente estado: {next_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Visit MC para estimar $V$\n",
    "\n",
    "El primer algoritmo que vamos a implementar es el de Monte Carlo First Visit. Este algoritmo estima la función de valor $V$ para una política dada.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Algoritmo First-Visit MC (para estimar }v_\\pi\\text{):} \\\\[6pt]\n",
    "\\textbf{Entrada:} & \\quad \\pi,\\ \\text{política a evaluar.}\\\\\n",
    "& \\quad \\text{(Se asume que podemos generar episodios con } \\pi\\text{.)}\\\\[6pt]\n",
    "\\textbf{Inicializar:} \n",
    "& \\quad V(s) \\in \\mathbb{R}, \\quad \\forall\\, s \\in S \\quad (\\text{valores arbitrarios})\\\\\n",
    "& \\quad \\text{Returns}(s) \\leftarrow \\varnothing, \\quad \\forall\\, s \\in S.\\\\[6pt]\n",
    "\\textbf{Bucle (por cada episodio):} \n",
    "& \\\\[-2pt]\n",
    "& \\quad \\text{Generar un episodio siguiendo } \\pi: (S_0, A_0, R_1, \\ldots, S_T).\\\\\n",
    "& \\quad G \\leftarrow 0.\\\\\n",
    "& \\quad \\textbf{para } t = T-1,\\, T-2,\\, \\ldots,\\, 0:\\\\\n",
    "& \\quad\\quad G \\leftarrow \\gamma\\,G + R_{t+1}.\\\\\n",
    "& \\quad\\quad \\textbf{si } S_t \\notin \\{S_0, S_1, \\ldots, S_{t-1}\\}:\\\\\n",
    "& \\quad\\quad\\quad \\text{Returns}(S_t) \\leftarrow \\text{Returns}(S_t) \\cup \\{G\\}.\\\\\n",
    "& \\quad\\quad\\quad V(S_t) \\leftarrow \\text{average}\\bigl(\\text{Returns}(S_t)\\bigr).\\\\[6pt]\n",
    "\\textbf{Retornar:} \n",
    "& \\quad V.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: con [defaultdict](https://docs.python.org/3/library/collections.html#defaultdict-objects) podemos definir un diccionario con un valor por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "A = defaultdict(float)\n",
    "B = defaultdict(int)\n",
    "C = defaultdict(lambda: [])\n",
    "\n",
    "print(A[999])\n",
    "print(B[999])\n",
    "print(C[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction(policy, env, number_episodes, gamma=1.0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sab_value = first_visit_mc_prediction(sutton_policy, env, number_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sab_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_distribution(sab_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una estrategía más elaborada, basada en Edward Thorp, que se basa en la carta del dealer y la suma del jugador. La idea es que si el dealer tiene una carta baja (2-6) el jugador se queda si tiene 12 o más, si no pide carta. Si el dealer tiene una carta alta (7-10) el jugador se queda si tiene 17 o más, si no pide carta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thorp_policy(observation):\n",
    "    player_sum, dealer_card, usable_ace = observation\n",
    "    # Si la suma del jugador es menor a 12, nunca se pasa, así que se pide carta.\n",
    "    if player_sum < 12:\n",
    "        return HIT\n",
    "    # Para sumas entre 12 y 16, se aconseja quedarse si el dealer muestra una carta débil (2 a 6),\n",
    "    # ya que el dealer tiene más probabilidad de pasarse.\n",
    "    if 12 <= player_sum <= 16:\n",
    "        if 2 <= dealer_card <= 6:\n",
    "            return STICK\n",
    "        else:\n",
    "            return HIT\n",
    "    # Con una suma de 17 o mayor, es recomendable plantarse.\n",
    "    return STICK\n",
    "\n",
    "thorp_value = first_visit_mc_prediction(thorp_policy, env, number_episodes=500_000)\n",
    "\n",
    "plot_value_distribution(thorp_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) ¿Qué valores se esperan de $V$ si implementamos el algoritmo de Monte Carlo Every Visit en este caso? ¿Por qué?\n",
    "> 2) Definir una política diferente que tenga en cuenta el As usable y estimar su función de valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Visit MC para estimar $Q$ con exploración inicial\n",
    "\n",
    "Cuando queremos estimar la función de valor $Q$ para una política dada, tenemos el problema de que no tenemos una política exploratoria. Por ende podría no visitar todos los estados y acciones. Para resolver esto, podemos usar la técnica de exploración inicial donde se elige un estado aleatorio ($S_0$) y una acción aleatoria ($A_0$) y se juega un episodio desde ahí. Esto nos permite explorar el espacio de estados y acciones. \n",
    "\n",
    "> Notar que la política no cambia, solo se elige un estado y acción aleatorios para empezar a jugar.\n",
    "\n",
    "> El desafío en Gymnasium es que no podemos elegir el estado inicial. Pero podemos intentar \"hacer trampa\" y modificar su estado interno. Esto no es recomendable, pero para fines de este laboratorio lo vamos a hacer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Algoritmo First-Visit MC (para estimar }Q(s,a)\\text{ con exploración inicial):} \\\\[6pt]\n",
    "\\textbf{Entrada:} & \\quad \\pi,\\ \\text{la política a evaluar.}\\\\[6pt]\n",
    "\\textbf{Inicializar:} & \\quad Q(s,a) \\in \\mathbb{R}\\quad \\text{(valores arbitrarios)},\\quad \\forall\\, s \\in S,\\; a \\in A(s),\\\\\n",
    "& \\quad \\text{Returns}(s,a) \\leftarrow \\varnothing,\\quad \\forall\\, s \\in S,\\; a \\in A(s).\\\\[6pt]\n",
    "\\textbf{Loop (por cada episodio):} & \\\\[-2pt]\n",
    "& \\quad \\text{Elegir al azar un estado inicial } S_0 \\in S \\text{ y una acción } A_0 \\in A(S_0).\\\\\n",
    "& \\quad \\text{Generar un episodio siguiendo } \\pi: (S_0, A_0, R_1, S_1, A_1, \\ldots, S_T).\\\\\n",
    "& \\quad G \\leftarrow 0.\\\\\n",
    "& \\quad \\textbf{para } t = T-1,\\; T-2,\\; \\ldots,\\; 0: \\\\\n",
    "& \\quad\\quad G \\leftarrow \\gamma\\,G + R_{t+1}.\\\\\n",
    "& \\quad\\quad \\textbf{si } (S_t,A_t) \\notin \\{(S_0,A_0),\\,(S_1,A_1),\\,\\ldots,\\,(S_{t-1},A_{t-1})\\}: \\\\\n",
    "& \\quad\\quad\\quad \\text{Returns}(S_t,A_t) \\leftarrow \\text{Returns}(S_t,A_t) \\cup \\{G\\}.\\\\\n",
    "& \\quad\\quad\\quad Q(S_t,A_t) \\leftarrow \\text{average}\\Bigl(\\text{Returns}(S_t,A_t)\\Bigr).\\\\[6pt]\n",
    "\\textbf{Retornar:} & \\quad Q.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.toy_text.blackjack import BlackjackEnv\n",
    "\n",
    "# https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/blackjack.py\n",
    "# https://gymnasium.farama.org/api/wrappers/#gymnasium.Wrapper\n",
    "class BlackjackEnvWrapper(BlackjackEnv):\n",
    "    def set_state(self, state):\n",
    "        \"\"\"\n",
    "        Fija el estado inicial deseado.\n",
    "        state: (player_sum, dealer_card, usable_ace)\n",
    "        \"\"\"\n",
    "        player_sum, dealer_card, usable = state\n",
    "        # Configuramos la mano del jugador\n",
    "        if usable:\n",
    "            # Para tener un as usable, construimos una mano que contenga un 1 y otra carta que\n",
    "            # haga que la suma (1 + otra carta + 10) sea player_sum.\n",
    "            card = player_sum - 11\n",
    "            # Aseguramos que la otra carta esté en el rango permitido (1 a 10)\n",
    "            card = max(1, min(card, 10))\n",
    "            self.player = [1, card]\n",
    "        else:\n",
    "            # Sin as usable, buscamos dos cartas que sumen player_sum\n",
    "            # Esto es una simplificación; se podría mejorar para asegurarse de no incluir un 1.\n",
    "            card1 = player_sum // 2\n",
    "            card2 = player_sum - card1\n",
    "            card1 = max(2, min(card1, 10))\n",
    "            card2 = max(2, min(card2, 10))\n",
    "            self.player = [card1, card2]\n",
    "        \n",
    "        # Configuramos la mano del dealer de modo que su carta visible sea la deseada.\n",
    "        self.dealer = [dealer_card, np.random.randint(1, 11)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_with_initial_exploration(policy, env):\n",
    "    \"\"\"\n",
    "    Genera un episodio utilizando la política dada, con exploración inicial.\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    env.reset()\n",
    "    \n",
    "    # Escogemos un estado inicial aleatorio\n",
    "    player_sum = np.random.randint(4, 22)   # suma entre 4 y 21\n",
    "    dealer_card = np.random.randint(1, 11)     # carta del dealer entre 1 y 10\n",
    "    usable = np.random.randint(0, 2)           # 0 o 1\n",
    "    state = (player_sum, dealer_card, usable)\n",
    "    \n",
    "    # Usamos el método del wrapper para fijar el estado\n",
    "    env.set_state(state)\n",
    "    state = env._get_obs()\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapped = BlackjackEnvWrapper(env)\n",
    "generate_episode_with_initial_exploration(thorp_policy, env_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_q_prediction_ie(policy, env, number_episodes, gamma=1.0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sab_q_value = first_visit_mc_q_prediction_ie(sutton_policy, env_wrapped, number_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sab_q_value[(21, 5, 0), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sab_q_value[(21, 5, 0), 1] # Por que da esto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q_distribution(sab_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thorp_q_value = first_visit_mc_q_prediction_ie(thorp_policy, env_wrapped, number_episodes=500_000)\n",
    "plot_Q_distribution(thorp_q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tareas**\n",
    "\n",
    "1. Implementar el algoritmo de Monte Carlo First Visit para estimar la función de valor $V$ y $Q$ para la política definida.\n",
    "2. Comparar las estimaciones de la función de valor para distintos episodios (ej: 100, 10000, 50000, 500000). \n",
    "3. Elaborar una política diferente que tenga en cuenta el As usable y estimar su función de valor.\n",
    "4. [extra] Implementar el algoritmo un algoritmo de control Monte Carlo para estimar la politica óptima. Puede ser MC ES para control (cap 5.3 del libro) o on-policy MC control (cap 5.4 del libro). Graficar resultados y comparar con politicas anteriores. ¿ Puede asegurar que \"la casa siempre gana\" ? 🎰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f71abb0199b7f142754c9e06c101c2f524a4efa5699ddd250d1cd17342266c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
